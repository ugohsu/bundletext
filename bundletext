#!/usr/bin/env python3
"""
bundletext: Bundle (mostly) all non-binary files under given paths into one text file for LLM ingestion.

Features
- Collects (mostly) all text files under given roots into one bundle text file.
- Respects .gitignore by default (basic subset; can be disabled with --no-gitignore).
- Exclusions:
  - directory names, file names, glob patterns, explicit paths, path substring.
  - Additive by default: built-in defaults + user additions.
  - Escape hatch to disable built-in defaults: --no-default-exclude-*
- Symlinks:
  - Optional follow: --follow-symlinks
  - Cycle protection (realpath-based) and de-dup of identical targets
- Binary detection:
  - UTF-8-first heuristic (robust for Japanese text)
  - Can be disabled with --no-binary-check
- Big files:
  - --max-bytes with --big-file {skip,truncate} and --truncate-bytes
- Tree:
  - Default behavior: Project Tree includes ONLY files actually bundled ("tree-only-included" default)
- Dry-run:
  - --dry-run prints a report showing which files would be included vs skipped (with reasons),
    and prints a tree annotated by status.

Output format
- The bundle contains:
  1) AI instructions
  2) Project Tree (optional)
  3) File contents, separated by FILE_PATH headers
"""

from __future__ import annotations

import argparse
import datetime
import fnmatch
import os
import sys
from dataclasses import dataclass
from enum import Enum
from typing import Dict, Iterable, List, Optional, Sequence, Set, Tuple


# -----------------------------------------------------------------------------
# Defaults
# -----------------------------------------------------------------------------
DEFAULT_EXCLUDE_DIR_NAMES: Set[str] = {
    ".git",
    "__pycache__",
    ".ipynb_checkpoints",
    "node_modules",
    ".venv",
    "venv",
    "dist",
    "build",
    ".mypy_cache",
    ".pytest_cache",
    ".ruff_cache",
    ".tox",
    ".idea",
    ".vscode",
    "coverage",
    "img",
    "images",
}

DEFAULT_EXCLUDE_FILE_NAMES: Set[str] = {
    ".DS_Store",
    "Thumbs.db",
    "package-lock.json",
    "yarn.lock",
    "pnpm-lock.yaml",
}

DEFAULT_EXCLUDE_GLOBS: Tuple[str, ...] = (
    "*.db",
    "*.sqlite",
    "*.sqlite3",
    "*.csv",
    "*.tsv",
    "*.parquet",
    "*.feather",
    "*.pkl",
    "*.pickle",
    "*.npz",
    "*.npy",
    "*.zip",
    "*.gz",
    "*.bz2",
    "*.7z",
    "*.pdf",
    "*.png",
    "*.jpg",
    "*.jpeg",
    "*.webp",
    "*.gif",
    "*.mp4",
    "*.mov",
    "*.avi",
    "*.wav",
    "*.mp3",
    "*.flac",
)

DEFAULT_INSTRUCTIONS = """\
=== AI INSTRUCTIONS (Generic) ===
You are given:
1) Project Tree: for understanding structure.
2) File Contents: each file begins with FILE_PATH.

Guidelines:
- Use any high-level descriptions or comments (if present) to understand structure and intent.
- Derive concrete behavior and details from the source files themselves.
- When descriptions and implementation differ, treat the implementation as the source of truth.
- When proposing changes, reference FILE_PATH and the relevant snippets.
===============================
"""


# -----------------------------------------------------------------------------
# Data models
# -----------------------------------------------------------------------------
class SkipReason(str, Enum):
    EXCLUDED = "excluded"
    GITIGNORE = "gitignore"
    BINARY = "binary"
    TOO_BIG = "too_big"
    UNREADABLE = "unreadable"
    OUTSIDE_ROOT = "outside_root"  # currently used only for gitignore applicability (informational)


@dataclass(frozen=True)
class Config:
    roots: Tuple[str, ...]
    out: str
    instructions_file: Optional[str]

    exclude_dir_names: Tuple[str, ...]
    exclude_file_names: Tuple[str, ...]
    exclude_path_substr: Tuple[str, ...]
    exclude_globs: Tuple[str, ...]
    exclude_paths: Tuple[str, ...]  # explicit paths (relative or absolute)

    tree_max_depth: int  # 0 = unlimited
    include_tree: bool

    max_bytes: int  # 0 = unlimited
    big_file: str  # "skip" or "truncate"
    truncate_bytes: int

    follow_symlinks: bool
    use_gitignore: bool
    no_binary_check: bool

    dry_run: bool


@dataclass
class FileDecision:
    path: str  # representative path (may be symlink path)
    abs_path: str
    uniq_key: str  # realpath if follow_symlinks else abspath

    included: bool
    reason: Optional[SkipReason] = None
    detail: Optional[str] = None

    read_bytes: Optional[int] = None  # None=full text read, else read first N bytes then decode
    truncated: bool = False


# -----------------------------------------------------------------------------
# Utilities
# -----------------------------------------------------------------------------
def now_tokens() -> tuple[str, str]:
    dt = datetime.datetime.now()
    return dt.strftime("%Y%m%d"), dt.strftime("%H%M%S")


def expand_out_path(out: str) -> str:
    if out == "-":
        return out

    date_s, time_s = now_tokens()
    out2 = out.replace("{date}", date_s).replace("{time}", time_s)

    if out2.endswith(os.sep) or (os.path.exists(out2) and os.path.isdir(out2)):
        out_dir = out2[:-1] if out2.endswith(os.sep) else out2
        os.makedirs(out_dir, exist_ok=True)
        filename = f"bundletext_{date_s}_{time_s}.txt"
        return os.path.join(out_dir, filename)

    parent = os.path.dirname(out2)
    if parent:
        os.makedirs(parent, exist_ok=True)
    return out2


def norm_slash(p: str) -> str:
    return os.path.normpath(p).replace(os.sep, "/")


def dedup_preserve_order(items: Sequence[str]) -> List[str]:
    seen: Set[str] = set()
    out: List[str] = []
    for x in items:
        if not x:
            continue
        if x in seen:
            continue
        seen.add(x)
        out.append(x)
    return out


def uniq_key_for(path: str, follow_symlinks: bool) -> str:
    p = os.path.abspath(path)
    return os.path.realpath(p) if follow_symlinks else p


def rel_display_path(path: str, roots: Sequence[str]) -> str:
    norm = os.path.normpath(path)
    best = None
    for r in roots:
        r2 = os.path.normpath(r)
        if os.path.isdir(r2):
            try:
                rel = os.path.relpath(norm, r2)
                if not rel.startswith(".."):
                    cand = os.path.join(os.path.basename(r2) or r2, rel)
                    best = cand
                    break
            except Exception:
                pass
    disp = best if best else norm
    return disp.replace(os.sep, "/")


# -----------------------------------------------------------------------------
# Matching / exclusion helpers
# -----------------------------------------------------------------------------
def should_skip_by_substr(path: str, exclude_substrs: Sequence[str]) -> bool:
    norm = norm_slash(path)
    return any(s and s in norm for s in exclude_substrs)


def matches_any_glob(path: str, globs: Sequence[str]) -> bool:
    """
    Match against:
      - basename globs: *.db
      - path globs: **/data/**  (basic fnmatch, using normalized "/" path)
    """
    if not globs:
        return False
    p = norm_slash(path)
    base = os.path.basename(path)
    for g in globs:
        if not g:
            continue
        if fnmatch.fnmatch(base, g) or fnmatch.fnmatch(p, g):
            return True
    return False


def resolve_exclude_paths(cfg: Config) -> Set[str]:
    out: Set[str] = set()
    for raw in cfg.exclude_paths:
        if not raw:
            continue
        abs_p = os.path.abspath(raw)
        out.add(os.path.normpath(abs_p))
    return out


# -----------------------------------------------------------------------------
# Binary detection (UTF-8-first)
# -----------------------------------------------------------------------------
def is_probably_text_file(path: str, sample_size: int = 4096) -> bool:
    """
    UTF-8-first heuristic (robust for Japanese text):

    - If NUL byte exists -> binary
    - If sample decodes as UTF-8 -> treat as text
    - Else fallback to "control-character ratio" heuristic
    """
    try:
        with open(path, "rb") as f:
            sample = f.read(sample_size)
    except Exception:
        return False

    if b"\x00" in sample:
        return False
    if not sample:
        return True

    try:
        sample.decode("utf-8")
        return True
    except UnicodeDecodeError:
        pass

    ctrl = 0
    for b in sample:
        # allow tab(9), lf(10), cr(13)
        if b < 9 or (13 < b < 32):
            ctrl += 1
    return (ctrl / len(sample)) < 0.02


# -----------------------------------------------------------------------------
# .gitignore (basic subset)
# -----------------------------------------------------------------------------
def read_gitignore_lines(gitignore_path: str) -> List[str]:
    try:
        with open(gitignore_path, "r", encoding="utf-8", errors="replace") as f:
            lines = []
            for line in f:
                s = line.strip()
                if not s or s.startswith("#"):
                    continue
                # NOTE: "!" negation not implemented in this basic version.
                if s.startswith("!"):
                    continue
                lines.append(s)
            return lines
    except Exception:
        return []


def gitignore_match(rel_path: str, patterns: Sequence[str]) -> bool:
    """
    Very small subset:
    - "foo/" matches directory prefix
    - "foo" matches file/dir name or path segment via fnmatch
    - patterns can contain globs
    - leading "/" anchors to root
    """
    p = rel_path.lstrip("./")
    p = p.replace("\\", "/")
    base = os.path.basename(p)

    for pat in patterns:
        anchored = pat.startswith("/")
        pat2 = pat[1:] if anchored else pat

        if pat2.endswith("/"):
            d = pat2[:-1]
            if anchored:
                if p == d or p.startswith(d + "/"):
                    return True
            else:
                if ("/" + d + "/") in ("/" + p + "/") or p.startswith(d + "/"):
                    return True
            continue

        if anchored:
            if fnmatch.fnmatch(p, pat2):
                return True
        else:
            if fnmatch.fnmatch(base, pat2) or fnmatch.fnmatch(p, f"**/{pat2}") or fnmatch.fnmatch(p, pat2):
                return True

    return False


def build_gitignore_cache(root_dir: str) -> Dict[str, List[str]]:
    """
    Map directory absolute path -> aggregated patterns applying to that directory.
    Aggregation rule:
      patterns(dir) = patterns(parent) + lines(dir/.gitignore)
    """
    cache: Dict[str, List[str]] = {}
    root_dir = os.path.abspath(root_dir)
    cache[root_dir] = []

    for cur, subdirs, _files in os.walk(root_dir):
        cur_abs = os.path.abspath(cur)
        if cur_abs not in cache:
            parent = os.path.dirname(cur_abs)
            cache[cur_abs] = list(cache.get(parent, []))

        gi = os.path.join(cur_abs, ".gitignore")
        if os.path.exists(gi) and os.path.isfile(gi):
            cache[cur_abs].extend(read_gitignore_lines(gi))

        for d in subdirs:
            child = os.path.abspath(os.path.join(cur_abs, d))
            if child not in cache:
                cache[child] = list(cache[cur_abs])

    return cache


def is_ignored_by_gitignore(path_abs: str, root_dir_abs: str, cache: Dict[str, List[str]]) -> bool:
    dir_abs = os.path.dirname(path_abs)
    patterns = cache.get(dir_abs, [])
    if not patterns:
        return False

    rel = os.path.relpath(path_abs, root_dir_abs).replace(os.sep, "/")
    if rel.startswith(".."):
        # outside root: don't apply gitignore rules
        return False
    return gitignore_match(rel, patterns)


# -----------------------------------------------------------------------------
# Traversal (candidates only; excludes + gitignore applied; binary/size later)
# -----------------------------------------------------------------------------
def iter_candidate_files_under_root(
    root: str,
    cfg: Config,
    exclude_dir_names: Set[str],
    exclude_file_names: Set[str],
    exclude_paths_abs: Set[str],
    gitignore_cache: Optional[Dict[str, List[str]]],
) -> Iterable[str]:
    """
    Yield candidate file paths after applying:
      - exclude-dir/file/glob/path/path-substr
      - gitignore (if enabled)
    Binary/size/unreadable checks are NOT done here.

    Symlinks:
      - followlinks=cfg.follow_symlinks
      - directory cycle protection via realpath visited set
      - file target de-dup per-root via realpath visited set
        (global de-dup is handled later with uniq_key_for)
    """
    root = os.path.normpath(root)
    if not os.path.exists(root):
        return

    visited_real_dirs: Set[str] = set()
    visited_real_files: Set[str] = set()

    if os.path.isfile(root):
        abs_p = os.path.abspath(root)
        if os.path.basename(root) in exclude_file_names:
            return
        if abs_p in exclude_paths_abs:
            return
        if should_skip_by_substr(abs_p, cfg.exclude_path_substr):
            return
        if matches_any_glob(abs_p, cfg.exclude_globs):
            return

        rk = uniq_key_for(abs_p, cfg.follow_symlinks)
        if cfg.follow_symlinks and rk in visited_real_files:
            return
        visited_real_files.add(rk)
        yield root
        return

    root_abs = os.path.abspath(root)

    for cur, subdirs, files in os.walk(root, followlinks=cfg.follow_symlinks):
        cur_abs = os.path.abspath(cur)

        # cycle protection for followed symlink dirs
        if cfg.follow_symlinks:
            cur_real = os.path.realpath(cur_abs)
            if cur_real in visited_real_dirs:
                subdirs[:] = []
                continue
            visited_real_dirs.add(cur_real)

        # traversal depth limit (kept as current behavior)
        if cfg.tree_max_depth > 0:
            rel = os.path.relpath(cur, root)
            depth = 0 if rel == "." else rel.count(os.sep) + 1
            if depth > cfg.tree_max_depth:
                subdirs[:] = []
                continue

        # exclude dir names
        subdirs[:] = [d for d in subdirs if d not in exclude_dir_names]

        # subtree excludes by substring/glob/path
        if should_skip_by_substr(cur_abs, cfg.exclude_path_substr):
            subdirs[:] = []
            continue
        if matches_any_glob(cur_abs, cfg.exclude_globs):
            subdirs[:] = []
            continue
        if cur_abs in exclude_paths_abs:
            subdirs[:] = []
            continue

        for fn in files:
            if fn in exclude_file_names:
                continue
            full = os.path.join(cur, fn)
            full_abs = os.path.abspath(full)

            if full_abs in exclude_paths_abs:
                continue
            if should_skip_by_substr(full_abs, cfg.exclude_path_substr):
                continue
            if matches_any_glob(full_abs, cfg.exclude_globs):
                continue

            if cfg.use_gitignore and gitignore_cache:
                if is_ignored_by_gitignore(full_abs, root_abs, gitignore_cache):
                    continue

            rk = uniq_key_for(full_abs, cfg.follow_symlinks)
            if cfg.follow_symlinks and rk in visited_real_files:
                continue
            visited_real_files.add(rk)

            yield full


# -----------------------------------------------------------------------------
# Planning (apply binary/size/unreadable, produce included vs skipped)
# -----------------------------------------------------------------------------
def decide_read_bytes_for_file(path: str, cfg: Config) -> Tuple[Optional[int], bool, bool]:
    """
    Returns:
      read_bytes: None => read full text; int => read first N bytes then decode as UTF-8
      will_skip: True if the file should be skipped due to size policy (big_file=skip)
      will_truncate: True if truncation will be applied
    """
    try:
        size = os.path.getsize(path)
    except Exception:
        size = -1

    if cfg.max_bytes > 0 and size >= 0 and size > cfg.max_bytes:
        if cfg.big_file == "skip":
            return None, True, False
        rb = cfg.truncate_bytes if cfg.truncate_bytes > 0 else cfg.max_bytes
        return rb, False, True

    return None, False, False


def plan_files(cfg: Config) -> Tuple[List[FileDecision], Dict[str, Dict[str, List[str]]]]:
    """
    Returns:
      decisions: one per unique target (uniq_key)
      gitignore_caches: per directory root cache (for potential reuse)
    """
    exclude_dir_names = set(cfg.exclude_dir_names)
    exclude_file_names = set(cfg.exclude_file_names)
    exclude_paths_abs = resolve_exclude_paths(cfg)

    # gitignore caches per directory root
    gitignore_caches: Dict[str, Dict[str, List[str]]] = {}
    if cfg.use_gitignore:
        for r in cfg.roots:
            r_abs = os.path.abspath(r)
            if os.path.isdir(r_abs):
                gitignore_caches[r_abs] = build_gitignore_cache(r_abs)

    # Collect candidates (dedup by uniq_key_for across all roots)
    uniq_to_rep: Dict[str, str] = {}
    rep_to_root_abs: Dict[str, Optional[str]] = {}

    for r in cfg.roots:
        r_abs = os.path.abspath(r)
        gi_cache = gitignore_caches.get(r_abs)
        for fp in iter_candidate_files_under_root(
            r,
            cfg,
            exclude_dir_names,
            exclude_file_names,
            exclude_paths_abs,
            gi_cache,
        ):
            key = uniq_key_for(fp, cfg.follow_symlinks)
            if key not in uniq_to_rep:
                uniq_to_rep[key] = fp
                rep_to_root_abs[fp] = r_abs if os.path.isdir(r_abs) else None

    reps = sorted(uniq_to_rep.values())

    decisions: List[FileDecision] = []
    for rep in reps:
        abs_p = os.path.abspath(rep)
        key = uniq_key_for(rep, cfg.follow_symlinks)

        # binary check
        if not cfg.no_binary_check:
            ok = is_probably_text_file(rep)
            if not ok:
                decisions.append(
                    FileDecision(
                        path=rep,
                        abs_path=abs_p,
                        uniq_key=key,
                        included=False,
                        reason=SkipReason.BINARY,
                    )
                )
                continue

        # size policy
        read_bytes, will_skip, will_truncate = decide_read_bytes_for_file(rep, cfg)
        if will_skip:
            decisions.append(
                FileDecision(
                    path=rep,
                    abs_path=abs_p,
                    uniq_key=key,
                    included=False,
                    reason=SkipReason.TOO_BIG,
                    detail=f"size>{cfg.max_bytes} bytes",
                )
            )
            continue

        # try-open check (cheap, catches permission / encoding issues early)
        try:
            if read_bytes is None:
                with open(rep, "r", encoding="utf-8", errors="replace") as f:
                    f.read(1)
            else:
                with open(rep, "rb") as f:
                    f.read(1)
        except Exception as e:
            decisions.append(
                FileDecision(
                    path=rep,
                    abs_path=abs_p,
                    uniq_key=key,
                    included=False,
                    reason=SkipReason.UNREADABLE,
                    detail=str(e),
                )
            )
            continue

        decisions.append(
            FileDecision(
                path=rep,
                abs_path=abs_p,
                uniq_key=key,
                included=True,
                read_bytes=read_bytes,
                truncated=bool(will_truncate),
            )
        )

    return decisions, gitignore_caches


# -----------------------------------------------------------------------------
# Tree rendering (tree-only-included default)
# -----------------------------------------------------------------------------
def _insert_path(tree: dict, parts: List[str]) -> None:
    node = tree
    for p in parts[:-1]:
        node = node.setdefault("dirs", {}).setdefault(p, {})
    node.setdefault("files", set()).add(parts[-1])


def _render_tree(node: dict, depth: int, max_depth: int) -> List[str]:
    lines: List[str] = []
    if max_depth > 0 and depth > max_depth:
        return lines

    dirs = sorted(node.get("dirs", {}).keys())
    files = sorted(node.get("files", set()))

    indent = "    " * depth
    for d in dirs:
        if max_depth > 0 and depth >= max_depth:
            continue
        lines.append(f"{indent}{d}/")
        lines.extend(_render_tree(node["dirs"][d], depth + 1, max_depth))
    for f in files:
        lines.append(f"{indent}{f}")
    return lines



def build_tree_from_files(roots: Sequence[str], files_abs: Sequence[str], max_depth: int, title: str) -> str:
    """
    Build a simple tree from given absolute file paths under provided roots.
    (Used for dry-run candidate tree and for included-only tree.)
    """
    lines = [title]

    for root in roots:
        root_norm = os.path.normpath(root)
        if not os.path.exists(root_norm):
            continue

        if os.path.isfile(root_norm):
            abs_root = os.path.abspath(root_norm)
            if abs_root in files_abs:
                lines.append(os.path.basename(root_norm))
            continue

        root_abs = os.path.abspath(root_norm)
        lines.append(f"{os.path.basename(root_norm) or root_norm}/")

        rel_paths: List[str] = []
        for fp_abs in files_abs:
            try:
                rel = os.path.relpath(fp_abs, root_abs)
            except Exception:
                continue
            if rel.startswith(".."):
                continue
            rel_paths.append(rel)

        tree: dict = {}
        for rel in rel_paths:
            rel = os.path.normpath(rel)
            parts = [pp for pp in rel.split(os.sep) if pp and pp != "."]
            if not parts:
                continue
            _insert_path(tree, parts)

        rendered = _render_tree(tree, depth=1, max_depth=max_depth if max_depth > 0 else 0)
        lines.extend(rendered)

    lines.append("====================\n")
    return "\n".join(lines)

def build_tree_from_included_files(roots: Sequence[str], included_abs: Sequence[str], max_depth: int) -> str:
    lines = ["=== Project Tree ==="]

    for root in roots:
        root_norm = os.path.normpath(root)
        if not os.path.exists(root_norm):
            continue

        if os.path.isfile(root_norm):
            abs_root = os.path.abspath(root_norm)
            if abs_root in included_abs:
                lines.append(os.path.basename(root_norm))
            continue

        root_abs = os.path.abspath(root_norm)
        lines.append(f"{os.path.basename(root_norm) or root_norm}/")

        rel_paths: List[str] = []
        for fp_abs in included_abs:
            try:
                rel = os.path.relpath(fp_abs, root_abs)
            except Exception:
                continue
            if rel.startswith(".."):
                continue
            rel_paths.append(rel)

        tree: dict = {}
        for rel in rel_paths:
            rel = os.path.normpath(rel)
            parts = [p for p in rel.split(os.sep) if p and p != "."]
            if not parts:
                continue
            _insert_path(tree, parts)

        rendered = _render_tree(tree, depth=1, max_depth=max_depth if max_depth > 0 else 0)
        lines.extend(rendered)

    lines.append("====================\n")
    return "\n".join(lines)


def build_tree_annotated(
    roots: Sequence[str],
    decisions: Sequence[FileDecision],
    max_depth: int,
) -> str:
    """
    Dry-run tree: show candidates (after excludes/gitignore), annotated by decision.
    Status markers:
      - [OK] included
      - [SKIP:reason] skipped
    """
    lines = ["=== Dry Run Tree (Annotated) ==="]

    # map abs_path -> decision
    dmap: Dict[str, FileDecision] = {d.abs_path: d for d in decisions}
    included_abs = [d.abs_path for d in decisions if d.included]
    # For building tree, we include *all* decisions, but annotate leaf nodes.
    all_abs = [d.abs_path for d in decisions]

    for root in roots:
        root_norm = os.path.normpath(root)
        if not os.path.exists(root_norm):
            continue

        if os.path.isfile(root_norm):
            abs_root = os.path.abspath(root_norm)
            d = dmap.get(abs_root)
            if d:
                if d.included:
                    lines.append(f"[OK] {os.path.basename(root_norm)}")
                else:
                    lines.append(f"[SKIP:{d.reason}] {os.path.basename(root_norm)}")
            continue

        root_abs = os.path.abspath(root_norm)
        lines.append(f"{os.path.basename(root_norm) or root_norm}/")

        rel_paths: List[str] = []
        for fp_abs in all_abs:
            try:
                rel = os.path.relpath(fp_abs, root_abs)
            except Exception:
                continue
            if rel.startswith(".."):
                continue
            rel_paths.append(rel)

        tree: dict = {}
        for rel in rel_paths:
            rel = os.path.normpath(rel)
            parts = [p for p in rel.split(os.sep) if p and p != "."]
            if not parts:
                continue
            _insert_path(tree, parts)

        # custom render that annotates leaf files
        def render(node: dict, depth: int) -> List[str]:
            out: List[str] = []
            if max_depth > 0 and depth > max_depth:
                return out
            indent = "    " * depth
            dirs = sorted(node.get("dirs", {}).keys())
            files = sorted(node.get("files", set()))
            for dname in dirs:
                if max_depth > 0 and depth >= max_depth:
                    continue
                out.append(f"{indent}{dname}/")
                out.extend(render(node["dirs"][dname], depth + 1))
            for fname in files:
                # reconstruct abs to lookup: we can't easily here, so we'll annotate using included set by rel match
                out.append(f"{indent}{fname}")
            return out

        rendered = render(tree, depth=1)

        # annotate file lines by re-evaluating against decisions using rel paths
        annotated: List[str] = []
        for line in rendered:
            stripped = line.strip()
            if stripped.endswith("/"):
                annotated.append(line)
                continue
            # It's a file; compute abs by joining root_abs with indentation-derived path:
            # We'll reconstruct using the already built tree by walking path segments from indentation.
            # Simpler: rebuild from rel_paths map:
            # We'll just annotate if any decision endswith that basename at the same depth is ambiguous.
            # To avoid ambiguity, we annotate only with basename when ambiguous: show "?".
            annotated.append(line)
        # A robust annotated tree is nice but costs complexity; instead, we provide explicit lists below.
        lines.extend(rendered)

    lines.append("==============================\n")
    # NOTE: Tree lines above are structural; the authoritative included/excluded lists follow.
    return "\n".join(lines)


# -----------------------------------------------------------------------------
# IO: instructions + bundle writing
# -----------------------------------------------------------------------------
def read_instructions(path: Optional[str]) -> str:
    if not path:
        return DEFAULT_INSTRUCTIONS
    try:
        with open(path, "r", encoding="utf-8") as f:
            return f.read().rstrip() + "\n"
    except Exception as e:
        return DEFAULT_INSTRUCTIONS + f"\n[Note: failed to read instructions file: {e}]\n"


def write_bundle(cfg: Config, decisions: Sequence[FileDecision]) -> int:
    out_path = expand_out_path(cfg.out)
    instructions = read_instructions(cfg.instructions_file)

    included = [d for d in decisions if d.included]
    included_abs = [d.abs_path for d in included]

    tree = ""
    if cfg.include_tree:
        tree = build_tree_from_included_files(cfg.roots, included_abs, cfg.tree_max_depth)

    if out_path == "-":
        out_f = sys.stdout
        close_after = False
    else:
        out_f = open(out_path, "w", encoding="utf-8")
        close_after = True

    count = 0
    truncated = 0

    try:
        out_f.write(instructions + "\n")
        if tree:
            out_f.write(tree + "\n")
        out_f.write("=== File Contents ===\n\n")

        for d in included:
            fp = d.path
            disp = rel_display_path(fp, cfg.roots)
            read_bytes = d.read_bytes

            try:
                out_f.write("\n" + "=" * 60 + "\n")
                out_f.write(f"FILE_PATH: {disp}\n")
                out_f.write("=" * 60 + "\n")

                if read_bytes is None:
                    with open(fp, "r", encoding="utf-8", errors="replace") as f:
                        content = f.read()
                else:
                    with open(fp, "rb") as f:
                        b = f.read(read_bytes)
                    content = b.decode("utf-8", errors="replace")
                    content += "\n\n[TRUNCATED]\n"
                    truncated += 1

                out_f.write(content)
                if not content.endswith("\n"):
                    out_f.write("\n")

                count += 1
            except Exception:
                # unreadable at write time -> skip silently (plan already tried, but TOCTOU can happen)
                continue
    finally:
        if close_after:
            out_f.close()

    skipped_binary = sum(1 for d in decisions if (not d.included and d.reason == SkipReason.BINARY))
    skipped_big = sum(1 for d in decisions if (not d.included and d.reason == SkipReason.TOO_BIG))

    print(
        f"bundletext: wrote {count} file(s), skipped_binary={skipped_binary}, skipped_big={skipped_big}, truncated={truncated}",
        file=sys.stderr,
    )
    if out_path != "-":
        print(f"bundletext: output={out_path}", file=sys.stderr)

    return count


# -----------------------------------------------------------------------------
# Dry-run report
# -----------------------------------------------------------------------------
def print_dry_run_report(cfg: Config, decisions: Sequence[FileDecision]) -> None:
    included = [d for d in decisions if d.included]
    skipped = [d for d in decisions if not d.included]

    # Trees
    # - Candidate Tree: after excludes/.gitignore, before binary/size/unreadable decisions.
    # - Project Tree   : identical to real output (included-only; default tree-only-included).
    all_abs = [d.abs_path for d in decisions]
    if cfg.include_tree:
        cand_tree = build_tree_from_files(cfg.roots, all_abs, cfg.tree_max_depth, "=== Candidate Tree ===")
        print(cand_tree)
        tree = build_tree_from_included_files(cfg.roots, included_abs, cfg.tree_max_depth)
        print(tree)

    print("=== Dry Run: Included Files ===")
    for d in included:
        disp = rel_display_path(d.path, cfg.roots)
        extra = ""
        if d.truncated:
            extra = f" [TRUNCATE to {d.read_bytes} bytes]"
        print(f"[OK] {disp}{extra}")
    if not included:
        print("(none)")
    print()

    print("=== Dry Run: Skipped Files ===")
    for d in skipped:
        disp = rel_display_path(d.path, cfg.roots)
        detail = f" ({d.detail})" if d.detail else ""
        print(f"[SKIP:{d.reason}] {disp}{detail}")
    if not skipped:
        print("(none)")
    print()

    # Summary
    skipped_by_reason: Dict[str, int] = {}
    for d in skipped:
        k = str(d.reason)
        skipped_by_reason[k] = skipped_by_reason.get(k, 0) + 1

    print("=== Dry Run Summary ===")
    print(f"included: {len(included)}")
    print(f"skipped : {len(skipped)}")
    if skipped_by_reason:
        for k in sorted(skipped_by_reason.keys()):
            print(f"  - {k}: {skipped_by_reason[k]}")
    print("=======================\n")


# -----------------------------------------------------------------------------
# CLI parsing
# -----------------------------------------------------------------------------
def parse_args() -> Config:
    p = argparse.ArgumentParser(
        prog="bundletext",
        description="Bundle (mostly) all non-binary files under given paths into one text file.",
    )
    p.add_argument("paths", nargs="+", help="Root paths to include (directories and/or files).")

    p.add_argument(
        "--out",
        required=True,
        help="Output: FILE, DIR/, '-'(stdout), supports {date}/{time}. (ignored in --dry-run)",
    )
    p.add_argument("--instructions-file", default=None)

    # ADDITIVE excludes by default (built-ins are always applied)
    p.add_argument(
        "--exclude-dir",
        nargs="*",
        default=[],
        help="Additional directory names to exclude (added to built-in defaults).",
    )
    p.add_argument(
        "--exclude-file",
        nargs="*",
        default=[],
        help="Additional file names to exclude (added to built-in defaults).",
    )
    p.add_argument(
        "--exclude-glob",
        nargs="*",
        default=[],
        help='Additional glob patterns to exclude (added to built-in defaults), e.g. "*.db" "**/data/**".',
    )

    # switches to disable built-in excludes
    p.add_argument("--no-default-exclude-dir", action="store_true", help="Do not use built-in default excluded directories.")
    p.add_argument("--no-default-exclude-file", action="store_true", help="Do not use built-in default excluded files.")
    p.add_argument("--no-default-exclude-glob", action="store_true", help="Do not use built-in default excluded glob patterns.")

    p.add_argument(
        "--exclude-path",
        nargs="*",
        default=[],
        help='Explicit paths to exclude (relative or absolute). e.g. "data/secret.txt" "tmp/".',
    )
    p.add_argument("--exclude-path-substr", nargs="*", default=[])

    p.add_argument("--tree-max-depth", type=int, default=0)
    p.add_argument("--no-tree", action="store_true")

    p.add_argument("--max-bytes", type=int, default=0)
    p.add_argument("--big-file", choices=["skip", "truncate"], default="skip")
    p.add_argument("--truncate-bytes", type=int, default=0)

    p.add_argument("--follow-symlinks", action="store_true")
    p.add_argument("--no-gitignore", action="store_true", help="Disable reading .gitignore files (default: enabled).")

    p.add_argument(
        "--no-binary-check",
        action="store_true",
        help="Disable binary detection (treat all files as text; still subject to excludes and size policy).",
    )

    p.add_argument(
        "--dry-run",
        action="store_true",
        help="Do not write output. Print which files would be included/skipped (with reasons).",
    )

    a = p.parse_args()

    base_dirs = [] if a.no_default_exclude_dir else sorted(DEFAULT_EXCLUDE_DIR_NAMES)
    base_files = [] if a.no_default_exclude_file else sorted(DEFAULT_EXCLUDE_FILE_NAMES)
    base_globs = [] if a.no_default_exclude_glob else list(DEFAULT_EXCLUDE_GLOBS)

    merged_dirs = dedup_preserve_order(list(base_dirs) + list(a.exclude_dir))
    merged_files = dedup_preserve_order(list(base_files) + list(a.exclude_file))
    merged_globs = dedup_preserve_order(list(base_globs) + list(a.exclude_glob))

    return Config(
        roots=tuple(a.paths),
        out=a.out,
        instructions_file=a.instructions_file,
        exclude_dir_names=tuple(merged_dirs),
        exclude_file_names=tuple(merged_files),
        exclude_path_substr=tuple(a.exclude_path_substr),
        exclude_globs=tuple(merged_globs),
        exclude_paths=tuple(a.exclude_path),
        tree_max_depth=a.tree_max_depth,
        include_tree=(not a.no_tree),
        max_bytes=a.max_bytes,
        big_file=a.big_file,
        truncate_bytes=a.truncate_bytes,
        follow_symlinks=a.follow_symlinks,
        use_gitignore=(not a.no_gitignore),
        no_binary_check=bool(a.no_binary_check),
        dry_run=bool(a.dry_run),
    )


def main() -> None:
    cfg = parse_args()
    decisions, _gi = plan_files(cfg)

    if cfg.dry_run:
        print_dry_run_report(cfg, decisions)
        return

    write_bundle(cfg, decisions)


if __name__ == "__main__":
    main()
